from nsfw_detector import predict
import profanity_check

from textblob import TextBlob

def profanity_check_text():
    text = "This is a sample text that may contain NSFW language."
    profanity_prob = profanity_check.predict_prob([text])[0]

    if profanity_prob > 0.5:
        print("The text is classified as containing profanity.")
    else:
        print("The text is clean.")


def sentiment_check():
    text = "Such a bad movie! Such a pathetic movie. Thinks can be good."
    blob = TextBlob(text)

    polarity = blob.sentiment.polarity
    print(polarity)
if polarity > 0:
        print("The sentiment is positive.")
    elif polarity < 0:
        print("The sentiment is negative.")
    else:
        print("The sentiment is neutral.")


def print_hi(name):
    model = predict.load_model('./nsfw_mobilenet2.224x224.h5')
    image_path = 'naked.jpg'
    prediction = predict.classify(model, image_path)
    print(prediction[image_path])
    is_nsfw = classify_nsfw(prediction['naked.jpg'], threshold=0.7)

    print(is_nsfw)


def classify_nsfw(result, threshold):
    nsfw_categories = ['hentai', 'porn', 'sexy']
    for category in nsfw_categories:
        score = result.get(category, 0.0)
        if score > threshold:
            return True
    return False


if __name__ == '__main__':
    print_hi('PyCharm')
    sentiment_check()
    profanity_check()
